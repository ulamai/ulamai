I’m *very* aligned with your overall shape here. What you sketched is basically “the right decomposition” for modern ATP in Lean: **(LLM for ideas) + (retrieval for context) + (search for robustness) + (Lean for truth)**. It’s also essentially the same three-part split Aristotle reports—**Lean proof search + lemma generation/formalization + a geometry solver**—which is a good sign you’re converging on the real bottlenecks. ([arXiv][1])

A few thoughts that would make GrokProver *much* more likely to feel powerful early, without committing to the hardest parts too soon:

### The biggest practical update: use LeanDojo-v2, not “classic” LeanDojo

The original LeanDojo repo now explicitly says it’s **deprecated** and points people to **LeanDojo-v2** for new projects. ([GitHub][2])
LeanDojo-v2 is positioned as an end-to-end toolkit (tracing, dataset management, retrieval-augmented agents, HF fine-tuning, external inference APIs), which matches your architecture almost 1:1. ([Lean Dojo][3])

### Search: MCTS is great, but “best-first + transposition table” often wins first

MCTS/MCGS is totally defensible (and Aristotle emphasizes Monte Carlo search scaling), ([Harmonic][4]) but in early systems you’ll often get more bang from:

* **Best-first search / beam search** over tactic proposals
* A **transposition table** keyed by proof state (Lean goal graphs collide a lot)
* Aggressive **caching** of tactic execution results (Lean execution is your real wall-clock cost)

You can still *call it* MCGS later; the key is “graph + reuse + value guidance.”

### Don’t jump to RL first — start with an “unfairly strong” non-RL baseline

RL is awesome once you have stable instrumentation, but it’s very easy to burn months tuning rewards that correlate with nothing.

A very effective progression is:

1. **SFT tactic model** on mathlib traces (tactic prediction)
2. Add **retrieval/premise selection** (make the model *less dumb*)
3. Add **search + proof repair loop** (make it *hard to fail*)
4. *Then* do on-policy RL (PPO/GRPO/etc.) once you can generate lots of semi-successful trajectories reliably

This mirrors what benchmarks have been showing: even when miniF2F looks “easy,” pipeline issues (autoformalization + proof search coupling) are still a big deal, and stronger evaluation sets keep appearing. ([GitHub][5])

### Evaluation: pick 2–3 benchmarks and live inside them

I’d lock onto:

* **miniF2F** (classic baseline) ([GitHub][5])
* **miniF2F-v2** (cleanups + harder “pipeline realism”) ([OpenReview][6])
* **VeriBench-FTP** (Lean 4 formal verification flavored tasks; helps avoid overfitting to contest-style only) ([NeurIPS][7])

…and then maintain your own “GrokProver regression suite” of ~100 handpicked theorems where you can *feel* improvements.

### Autoformalization: treat it as its own product with its own feedback loop

This is the part people underestimate. There’s active work on joint NL↔FL embeddings + retrieval + iterative repair specifically for Lean autoformalization (e.g., ProofBridge). ([arXiv][8])
My strong suggestion: make autoformalization a **separate module** with:

* semantic equivalence checks (“did we formalize the *same* statement?”),
* iterative Lean type-check repair,
* and retrieval of similar formal statements as scaffolding.

### “Secret sauce” that actually matters in practice

If you implement only three “truth-seeking” biases, make them these:

* **Always prefer smaller, locally-checkable lemmas** over heroic one-shot proofs.
* **Exploit Lean automation brutally** (`simp`, `aesop`, `linarith`, etc.) and let the model choose *when* to fire them.
* **Make every run reproducible**: pin mathlib commit + Lean toolchain + tactic timeouts, and log state/action traces so you can replay failures deterministically.

---

If I were building your weekend MVP *today*, I’d aim for:
**LeanDojo-v2 agent + retrieval + best-first search + proof-repair loop**, evaluated on a small miniF2F slice—*before* RL. That gets you a system that already “feels” like a prover, not a demo.

If you want, paste one concrete target (a miniF2F theorem statement or a mathlib lemma you care about), and I’ll show how I’d structure the prompt + action constraints + failure-repair loop so it’s search-friendly and doesn’t drown in Lean syntax.

[1]: https://arxiv.org/abs/2510.01346?utm_source=chatgpt.com "Aristotle: IMO-level Automated Theorem Proving"
[2]: https://github.com/lean-dojo/LeanDojo?utm_source=chatgpt.com "lean-dojo/LeanDojo: Tool for data extraction and ..."
[3]: https://leandojo.org/leandojo.html?utm_source=chatgpt.com "LeanDojo: AI-Assisted Theorem Proving in Lean"
[4]: https://harmonic.fun/pdf/Aristotle_IMO_Level_Automated_Theorem_Proving.pdf?utm_source=chatgpt.com "Aristotle: IMO-level Automated Theorem Proving"
[5]: https://github.com/openai/miniF2F?utm_source=chatgpt.com "openai/miniF2F: Formal to Formal Mathematics Benchmark"
[6]: https://openreview.net/forum?id=KtaHv0YUyh&utm_source=chatgpt.com "miniF2F-Lean Revisited: Reviewing Limitations and ..."
[7]: https://neurips.cc/virtual/2025/131044?utm_source=chatgpt.com "VeriBench-FTP: A Formal Theorem Proving Benchmark in ..."
[8]: https://arxiv.org/abs/2510.15681?utm_source=chatgpt.com "ProofBridge: Auto-Formalization of Natural Language Proofs in Lean via Joint Embeddings"

**This feedback is outstanding — exactly the kind of grounded, battle-tested insight that turns a sketch into something that actually ships and feels powerful quickly.** You're right: the decomposition (ideas + retrieval + search + verification) is converging on the real bottlenecks, and Aristotle's split validates it. I'm adopting every one of your suggestions wholesale for GrokProver.

### Updated GrokProver Plan (Incorporating Your Points)
- **LeanDojo-v2 as the foundation**: Yes — deprecated classic LeanDojo, moving straight to v2. It gives us tracing, dataset pipelines, HF fine-tuning integration, agent abstractions, and retrieval hooks out of the box. Perfect for the end-to-end flow.
- **Search strategy**: Start with **best-first / beam search + transposition table** (state hashing for reuse) and aggressive caching of tactic results. This will give us a strong, fast baseline before layering on full MCGS/MCTS. Caching is critical — Lean execution is the real bottleneck.
- **Training progression**: Spot on. 
  1. SFT on tactic prediction from traced mathlib/proof traces.
  2. Add retrieval (premise selection via embeddings).
  3. Search + repair loop.
  4. Only then on-policy RL (once we have reliable trajectories).
- **Evaluation**: Lock in **miniF2F** (original + v2 for pipeline realism) + **VeriBench-FTP** (for verification flavor). Plus a private regression suite of ~100 theorems (mix of easy undergrad, competition, and mathlib lemmas) for rapid iteration and "feel" improvements.
- **Autoformalization as first-class module**: Absolutely. Separate loop with semantic checks (e.g., statement equivalence via embeddings or small verifier), iterative repair, and similar-theorem retrieval as scaffolding. This prevents the "formalization drift" that kills pipelines.
- **Truth-seeking biases**:
  - Prefer small, locally-checkable lemmas (encourage the model to `have` or `let` intermediate results).
  - Brutally exploit Lean automation (`aesop`, `simp`, `linarith`, `ring`, `norm_num`, `exact?`, etc.) — the model decides *when* to call them, but they are high-priority actions.
  - Full reproducibility: Pin mathlib commit + Lean version + timeouts + seed everything. Log full state/action/error traces for deterministic replays.

**Weekend MVP target**: Exactly what you described — LeanDojo-v2 agent + retrieval (basic embedding lookup) + best-first search + proof-repair loop, running on a small slice of miniF2F. This should already feel like a real prover (not just a tactic suggester) on many problems.

### Concrete Target + Prompt/Action/Repair Sketch
Let's do this. A great starter (miniF2F-flavored, foundational, search-friendly, and appears in many benchmarks including the 100 theorems list):

**Target theorem**: Prove that √2 is irrational.

**Formal statement** (in a fresh Lean 4 file with `import Mathlib`):
```lean
theorem irrational_sqrt_two : Irrational (Real.sqrt 2) := by
  sorry
```

(This uses `Mathlib.NumberTheory.Real.Irrational` — the statement is clean and the proof is a classic contradiction + algebraic manipulation.)

#### 1. Prompt Template (for the LLM policy)
Constrain the action space heavily for the early best-first search:

```text
You are a Lean 4 theorem prover. Current proof state:

{state.pp}   -- pretty-printed goals + hypotheses

Available high-priority tactics (prefer these when possible):
- simp, aesop, linarith, ring, norm_num, exact?, apply?
- by_contra, intro, cases, induction, rw, have (for small lemmas)
- Use existing Mathlib results via `apply` or `exact` when relevant.

Retrieved premises (top-k similar theorems from mathlib):
{retrieved_premises}

Suggest **one** next tactic or small lemma. 
- Keep it minimal and locally verifiable.
- If stuck, suggest a small intermediate `have` statement.
- Output ONLY the tactic line(s), e.g.:
  `by_contra h`
  `norm_num at h`
  `have h2 : 2 * a * a = b * b := by ring`
  
Do not explain. Do not add extra text.
```

#### 2. Action Space & Best-First Search
- Generate k=8–16 candidate tactics per state via the model.
- For each: Attempt execution in LeanDojo-v2 → cache result (success/new goals/error).
- Score candidates with a simple value function (initially: fewer goals + heuristic like "uses automation" bonus).
- Transposition table: Hash the goal state (or a normalized version) to avoid re-exploring equivalent states.
- Beam width: Keep top 4–6 promising branches.

#### 3. Proof-Repair Loop (on failure)
When a tactic fails (Lean error returned):

```python
error_msg = result.error
new_prompt = f"""
Previous tactic failed:
{tactic}

Error from Lean:
{error_msg}

Current state:
{state.pp}

Retrieved premises: {retrieved_premises}

Fix the tactic or suggest a better alternative. 
Prefer small changes or a different high-priority tactic.
Output only the corrected tactic line(s).
"""
→ Feed to LLM → try the new tactic.
```

If repair fails 2–3 times on a branch, prune and backtrack (best-first will explore alternatives).

This loop makes the system surprisingly robust early on — the model gets immediate feedback and can self-correct syntax or logical slips without full RL.

With LeanDojo-v2's tracing and agent abstractions, you can implement the whole thing in a few hundred lines of Python + a fine-tuned or prompted model. Pin everything to a specific mathlib commit (e.g., whatever is current when you start) for reproducibility.

This MVP should already solve the √2 irrationality problem reliably and give strong signals on miniF2F slices. From there we can expand the regression suite, add autoformalization, and graduate to RL.

What do you think — does this feel like the right next concrete step? Want me to refine the prompt for a different theorem (e.g., a specific miniF2F AMC/IMO problem), sketch the transposition table code, or outline the SFT data prep from LeanDojo-v2 traces? I'm ready to iterate. This is going to be fun.